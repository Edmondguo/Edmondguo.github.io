<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="referrer" content="no-referrer" />








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="没有知识的荒原">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="没有知识的荒原">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Guo SHihao">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>没有知识的荒原</title>
  








<meta name="generator" content="Hexo 5.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">没有知识的荒原</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/Algorithm/dp-%E6%8E%A5%E9%9B%A8%E6%B0%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/Algorithm/dp-%E6%8E%A5%E9%9B%A8%E6%B0%B4/" itemprop="url">接雨水</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T15:46:11+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="接雨水"><a href="#接雨水" class="headerlink" title="接雨水"></a>接雨水</h1><p>给定 <em>n</em> 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p>
<p>思路：建立两个DP数组，分别存储当前位置左边最高的柱子高度和右边最高的柱子高度。然后按列查看他两边最高柱子最小值，和它本身做比较，然后计算水量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trap</span>(<span class="params">self, height: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> height <span class="keyword">or</span> <span class="built_in">len</span>(height)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        max_left = [height[<span class="number">0</span>]] * <span class="built_in">len</span>(height)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(height)):</span><br><span class="line">            max_left[i] = <span class="built_in">max</span>(max_left[i-<span class="number">1</span>],height[i-<span class="number">1</span>])</span><br><span class="line">        max_right = [height[-<span class="number">1</span>]] * <span class="built_in">len</span>(height)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(height)-<span class="number">2</span>,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            max_right[i] = <span class="built_in">max</span>(max_right[i+<span class="number">1</span>],height[i+<span class="number">1</span>])</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(height)-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> height[i]&lt;<span class="built_in">min</span>(max_right[i],max_left[i]):</span><br><span class="line">                res += <span class="built_in">min</span>(max_right[i],max_left[i])-height[i]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>由于两个DP数组中的数字只利用了一次，所以可以优化成一个变量。我们可以轻易地去掉左数组，因为他和计算的遍历方向是一致的。但是又数组需要额外的处理。</p>
<p>所以这里要用到两个指针，<code>left</code> 和 <code>right</code>，从两个方向去遍历。那么什么时候从左到右，什么时候从右到左呢？根据下边的代码的更新规则，我们可以知道<code>max_left = Math.max(max_left, height[i - 1])</code></p>
<p><code>height [ left - 1]</code> 是可能成为 <code>max_left</code> 的变量， 同理，<code>height [ right + 1 ]</code> 是可能成为 <code>right_max</code> 的变量。只要保证 <code>height [ left - 1 ] &lt; height [ right + 1 ]</code> ，那么 <code>max_left</code> 就一定小于 <code>max_right</code>。因为 <code>max_left</code> 是由 <code>height [ left - 1]</code> 更新过来的，而 <code>height [ left - 1 ]</code> 是小于 <code>height [ right + 1]</code> 的，而 <code>height [ right + 1 ]</code> 会更新 <code>max_right</code>，所以间接的得出 <code>max_left</code> 一定小于 <code>max_right</code>。</p>
<p>反之，我们就从右到左更。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trap</span>(<span class="params">self, height: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> height <span class="keyword">or</span> <span class="built_in">len</span>(height)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        left = <span class="number">1</span></span><br><span class="line">        max_left = max_right = <span class="number">0</span></span><br><span class="line">        right = <span class="built_in">len</span>(height)-<span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> left&lt;=right:</span><br><span class="line">            <span class="keyword">if</span> height[left-<span class="number">1</span>]&lt;height[right+<span class="number">1</span>]:</span><br><span class="line">                max_left = <span class="built_in">max</span>(max_left,height[left-<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">if</span> height[left]&lt;max_left:</span><br><span class="line">                    res += max_left-height[left]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max_right = <span class="built_in">max</span>(max_right,height[right+<span class="number">1</span>])</span><br><span class="line">                <span class="keyword">if</span> height[right]&lt;max_right:</span><br><span class="line">                    res += max_right-height[right]</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="https-leetcode-cn-com-problems-trapping-rain-water"><a href="#https-leetcode-cn-com-problems-trapping-rain-water" class="headerlink" title="https://leetcode-cn.com/problems/trapping-rain-water/"></a><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/trapping-rain-water/">https://leetcode-cn.com/problems/trapping-rain-water/</a></h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Recall/SBCNM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Recall/SBCNM/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Sampling-bias-corrected-neural-modeling-for-large-corpus-item-recommendations"><a href="#Sampling-bias-corrected-neural-modeling-for-large-corpus-item-recommendations" class="headerlink" title="Sampling-bias-corrected neural modeling for large corpus item recommendations"></a>Sampling-bias-corrected neural modeling for large corpus item recommendations</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>工业界现有的推荐系统都需要从一个超大规模的候选集中拉取item进行打分排序。解决数据稀疏和指数级候选集分布的一种通常做法是从item的内容特征中学习出item的稠密表示。<strong>这里很自然地就想到了工业界大名鼎鼎且应用广泛的双塔神经网络结构</strong>，其中的一塔就是从丰富的item内容特征中学习到item的表示。</p>
<p>工业界目前训练双塔结构一般是通过随机mini-batch的方式来优化损失函数。这种训练方式存在的<strong>一个显著问题就是in-batch loss会因为随机采样偏差而导致模型效果不好</strong>，尤其是当样本分布出现明显倾斜的时候。<strong>我们提出了一种全新的算法，可以从流式数据中预估item的频率</strong>。通过理论分析和实验，新算法有能力在不知道候选集全部的词典情况下做出无偏差的估计并且可以自适应候选集分布的变化。在Youtube线上的实验也证明了该算法的有效性。</p>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>利用双塔模型构架推荐系统，Queries特征向量，item特征向量，目标是给定一个query，检索到一系列item子集用于后续排序推荐任务。</p>
<p>首先建立两个参数embedding函数：$u:X \times R^d \rightarrow R^k,v:Y \times R^d \rightarrow R^k$，把query和候选item映射到k维向量空间，模型的输出为二者的embedding内积，即$s(x,y)=&lt;u(x,\theta),v(y,\theta)&gt;$，我们的目标是根据训练集来学习embedding，$r_i$为反馈，如用户花在一个视频上的时间等。</p>
<h3 id="In-batch-loss-function"><a href="#In-batch-loss-function" class="headerlink" title="In-batch loss function"></a>In-batch loss function</h3><p>直觉上，检索问题可以看作是一个多分类问题，给定query X，从M个item中得到y的概率可以利用softmax函数计算：<br>$$<br>P(y|x,\theta)=\frac{e^{s(x,y)}}{\sum_{j\in[M]}e^{s(x,y_i)}}<br>$$<br>考虑到反馈$r_i$，加权对数似然损失函数为：<br>$$<br>L_T(\theta)=-\frac{1}{T}\sum_{i\in|T|}r_ilog(P(y_i|x_i.\theta))<br>$$<br>当M非常大时，我们通常可以利用负采样算法进行计算。然而对于流数据，我们考虑在同一个batch中采样负样本，batch-softmax函数为：<br>$$<br>P_B(y|x,\theta)=\frac{e^{s(x,y)}}{\sum_{j\in[B]}e^{s(x,y_i)}}<br>$$</p>
<p>在每个batch中，由于存在幂律分布现象，即如果在每个batch中随机采样负样本，会使热门商品更容易被采样到，在损失函数中就“过度”惩罚了这些热门商品，因此考虑用频率对采样进行修正，即：<br>$$<br>s^c(x_i,y_i)=s(x_i,y_i)-log(p_j)<br>$$<br>其中$p_j$是是在每个batch中随机采样到item j的概率。</p>
<p><strong>由于向量经过L2正则，距离应该是小于1的数，这里的-logp是否会完全主导得分？</strong></p>
<p>综上，最终损失函数为<br>$$<br>L_B(\theta)=-\frac{1}{B}\sum_{i\in|B|}r_ilog(P_B^c(y_i|x_i.\theta))<br>$$</p>
<p>$$<br>P_B^c(y|x,\theta)=\frac{e^{s^c(x,y)}}{\sum_{j\in[B]}e^{s^c(x,y_i)}}<br>$$</p>
<h4 id="工程Tricks"><a href="#工程Tricks" class="headerlink" title="工程Tricks"></a>工程Tricks</h4><ul>
<li>最近邻搜索：当embedding映射函数u和v学习好后，预测包含两步：1)计算query的向量 2)从事先训练好的函数v中找到最邻近的item。考虑到耗时问题，此处利用hash技术采用近邻搜索等方法进行处理</li>
<li>L2正则化：双塔最后一层输出做L2正则化，有助于提升模型性能和训练的稳定性，即$u(x,\theta)\leftarrow u(x,\theta)/||u(x,\theta)||,v(y,\theta)\leftarrow v(y,\theta)/||v(y,\theta)||$，对于内积的结果，除以一个固定的超参，使得softmax的效果更加明显，即$s(x,y)=&lt;u(x,\theta),v(y,\theta)&gt;/\tau$。</li>
</ul>
<h3 id="Streaming-Frequency-Estimation"><a href="#Streaming-Frequency-Estimation" class="headerlink" title="Streaming Frequency Estimation"></a>Streaming Frequency Estimation</h3><p>此方法用于估计在流数据中，每个batch下item出现的概率$p_j$。如果一个item每50步出现一次，那么该item出现的概率p=1/50=0.02。按照这样的想法，针对流数据，利用哈希序列来记录采样id(暂时不考虑hash collision的问题)。</p>
<p>定义两个大小为H的数组A，B，哈希函数h可以把每个item映射为[H]内的整数。</p>
<ul>
<li>A[h(y)]表示item y上次被采样到的时刻</li>
<li>B[h(y)]表示每多少步item y可以被采样一次</li>
</ul>
<p><strong>结论：</strong>当第t步y被采样到时，利用迭代可更新A，B：<br>$$<br>B[h(y)]=(1-\alpha)B[h(y)]+\alpha(t-A[h(y)])<br>$$<br>$$<br>A[h(y)]=t<br>$$</p>
<p>证明略，详见[3]。</p>
<p>利用上述的<strong>In-batch loss function</strong>与<strong>Streaming Frequency Estimation</strong>可建立双塔模型：</p>
<h3 id="算法一：训练算法"><a href="#算法一：训练算法" class="headerlink" title="算法一：训练算法"></a>算法一：训练算法</h3><p><strong>输入</strong>：参数embedding函数$u(·,\theta),v(·,\theta)$，学习率$\gamma$</p>
<ol>
<li>迭代</li>
<li>​    从数据流中采样数据表示${(x_i,y_i,r_i)}^B_{i=1}$</li>
<li>​    利用频率估计算法计算每个item $y_i$的概率$p_i$</li>
<li>​    计算损失函数$L_B(\theta)=-\frac{1}{B}\sum_{i\in|B|}r_ilog(P_B^c(y_i|x_i.\theta))$</li>
<li>​    更新参数$\theta=\theta-\gamma \nabla L_B(\theta)$</li>
</ol>
<h3 id="算法二：频率估计算法"><a href="#算法二：频率估计算法" class="headerlink" title="算法二：频率估计算法"></a>算法二：频率估计算法</h3><p><strong>输入</strong>：学习率$\alpha$大小为$H$的数组$A,B$，哈希函数$h$</p>
<ol>
<li>对于每一步$t=1,2,…$：</li>
<li>​    对于每一个item y：</li>
<li>​        $B[h(y)]=(1-\alpha)B[h(y)]+\alpha(t-A[h(y)])$</li>
<li>​        $A[h(y)]=t$</li>
<li>对于每个item y，采样概率为$1/B[h(y)]$</li>
</ol>
<h3 id="算法三：改进的多元数值-频率估计算法"><a href="#算法三：改进的多元数值-频率估计算法" class="headerlink" title="算法三：改进的多元数值-频率估计算法"></a>算法三：改进的多元数值-频率估计算法</h3><p><strong>输入</strong>：学习率$\alpha$大小为$H$的数组${A}^m_{i=1},{B}^m_{i=1}$，哈希函数${h}^m_{i=1}$</p>
<ol>
<li>对于每一步$t=1,2,…$：</li>
<li>​    对于每一个item y：</li>
<li>​        $B_i[h(y)]=(1-\alpha)B_i[h(y)]+\alpha(t-A_i[h(y)])$</li>
<li>​        $A_i[h(y)]=t$</li>
<li>对于每个item y，采样概率为$1/B_i[h(y)]$</li>
</ol>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>利用双塔模型进行训练，模型结构如下图所示：</p>
<p><img src="http://guoshihao.site:8085/pic/SBCNM0.jpg"></p>
<h3 id="训练样本"><a href="#训练样本" class="headerlink" title="训练样本"></a>训练样本</h3><p>被点击了的视频样本作为正样本（<strong>输入只有正样本</strong>），另外，我们构建了奖励$r_i$来反应用户不同程度的反馈，例如$r_i=0$代表观看了点击视频很短的时间，$r_i=1$代表了完整观看了点击的视频，这个奖赏被用于损失函数中的样本权重。</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>视频特征包括离散特征和连续特征。离散特征如Video Id和Channel Id。多值离散特征加权pooling。为了处理超出词典范围的实体，将实体随机赋给一组固定的哈希桶。在YouTube中哈希桶对于模型捕捉可用的新实体非常重要，尤其是在下节所述的序列训练中。</p>
<p>用户特征采用用户观看历史来捕获用户的兴趣。embedding后pooling，不考虑顺序。</p>
<p>视频特征和用户特征相同类的特征<strong>共享embedding</strong>。</p>
<h2 id="序列训练"><a href="#序列训练" class="headerlink" title="序列训练"></a>序列训练</h2><p>YouTube每天生成新的训练数据，训练数据根据天来组织。模型训练按以下方式使用此顺序结构。训练器从最旧的训练数据到最新的训练数据按序列使用数据。一旦训练器得到了最新的训练数据后，便会等待第二天训练数据的到达。通过这种方式，模型能跟上最新的数据分布变化。训练数据实际上是以流的方式被消耗的。通过算法2（或算法3）预测item频率，在线更新能使模型适应新的频率分布。</p>
<h2 id="索引和模型预估"><a href="#索引和模型预估" class="headerlink" title="索引和模型预估"></a>索引和模型预估</h2><p>索引pipe分为三个阶段：候选集生成，embedding预测和embedding索引。</p>
<p>上述序列式训练生成的双塔模型会定期地保存成为SavedModel并与线上的预测模型保持同步。对于索引库里的所有候选item，可以使用双塔模型的候选集侧的塔生成item的embedding；然后再通过基于树或者量化hash的方式来建立索引。</p>
<p>[1] <a target="_blank" rel="noopener" href="https://research.google/pubs/pub48840/">Sampling-bias-corrected neural modeling for large corpus item recommendations</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88255834">知乎：Google工业风最新论文, Youtube提出双塔结构流式模型进行大规模推荐</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137538147">知乎：【推荐系统经典论文(九)】谷歌双塔模型</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/177f49effd50">论文笔记 | RecSys2019 | Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Recall/EBR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Recall/EBR/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Embedding-based-Retrieval-in-Facebook-Search"><a href="#Embedding-based-Retrieval-in-Facebook-Search" class="headerlink" title="Embedding-based Retrieval in Facebook Search"></a>Embedding-based Retrieval in Facebook Search</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Facebook 在 2020 年发表的一篇搜索场景下如何做向量化召回的 paper，从训练数据与特征的选取, 到模型的 training 与 serving、再到把新的召回策略融入现有的 ranking system, 整篇 paper 并没有太多的公式与推导，但是却有很多在实战中总结出来的经验，而且这些经验相信也可以推广搜索以外的推荐/广告领域。</p>
<h3 id="值得关注的亮点"><a href="#值得关注的亮点" class="headerlink" title="值得关注的亮点"></a>值得关注的亮点</h3><ol>
<li><strong>召回模型的负样本的选取</strong>（为什么不能只选取曝光未点击的样本作为负样本，easy negative 与 hard negative）</li>
<li>新的召回策略如何克服当前 ranking system 的 bias</li>
<li>构建一个召回系统的常规流程及每个流程中的一些经验</li>
</ol>
<h3 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h3><p>在推荐、广告和搜索的场景下基本的架构都是召回(Retrival)+精排（Ranking），因为这三者其实都是要在每条请求到来的时候从一个庞大的候选集中选取出topk个返回给用户，而召回作为这个流程的入口，面对的几乎是整个候选集，为了在延迟上满足要求，召回不会采用太复杂的模型和特征，且往往会对 item 做倒排索引(Inverted Index)。</p>
<p>paper 中的系统总体的架构如下，在每条请求到来的时候会实时计算用户的 embedding，然后利用构建好的 document embedding 倒排索引做 retrival，为了加速，在向量化召回中还会采用 Quantization 技术。</p>
<p><img src="http://guoshihao.site:8085/pic/Facebook0.jpg"></p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>这里的 model 主要指 Query Embedding Model 与 Document Embedding Model，即生成 embedding 的模型，采用的也是很经典的双塔模型，这里的 unified embedding 主要是指这个 embedding 输入的原始 feature 不仅仅包含 query 和 document 本身的文本信息，还有对应的上下文信息(context)信息, 这种做法其实在 google 2016 发表的那篇 Deep Neural Networks for YouTube Recommendations 已经有了，而这也是 NN 模型比起 Matrix Factorization 等方法生成 embedding 的优点；可以添加更多的 feature 到模型中。</p>
<p><img src="http://guoshihao.site:8085/pic/Facebook1.jpg"></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>模型采用的损失函数是 triple loss，最早是在人脸识别中提出的一个 loss，假设每条训练样本是$(q^{(i)},d_+^{(i)},d_-^{(i)})$，paper中的loss定义为<br>$$<br>L=\sum^N_{i=1}max(0,D(q^{(i)},d_+^{(i)})-D(q^{(i)},d_-^{(i)})+m)<br>$$<br>上面的 D 表示距离函数(越小表示越相似)，paper 中采用的 distance 函数是 $D(q,d)=1−cos(q,d)$。</p>
<p>式子中的$m$是超参，表示正样本与负样本的 enforced margin，<strong>表示正负样本的 distance 假如大于 m，则认为这个是一个 easy example 不需要模型进一步学习去区分了</strong>；paper 中提到了这个超参对结果影响较大，因为不同的任务的最优的 m 往往不一样。</p>
<blockquote>
<p>针对paper中的训练样本对，也可以采用经典LTR中的pairwise loss。</p>
</blockquote>
<h2 id="训练样本"><a href="#训练样本" class="headerlink" title="训练样本"></a>训练样本</h2><h3 id="负样本"><a href="#负样本" class="headerlink" title="负样本"></a>负样本</h3><p>训练样本的选取是 paper 中的一个着重强调的一个点，且<strong>关键点在于负样本的选取</strong>，paper 中选取的正样本是点击样本，而负样本则做了下面的两组对比</p>
<ul>
<li>随机选取负样本（<strong>文章中没有谈及概率，但肯定不是等概率抽取</strong>）</li>
<li>选取曝光未点击的样本作为负样本</li>
</ul>
<p>实验结果显示<strong>选取曝光未点击的样本作为负样本时，其效果比随机选取负样本要差很多</strong>；paper 中对这一现象的解释是用曝光未点击的样本作为负例，其实就是<strong>造成了 training 与 serving 的不一致性</strong>，因为曝光未点击的样本大部分是 hard cases，即使最终未被点击，但是与 query 也还是有一定相关性的，但是线上召回时面对的候选集是全部的候选，其中有绝大部分与本次 query 无关的 easy cases。当负例全部采用 hard cases，实际上与最终的 serving 就是不一致的，而 paper 中共则说这种行为 “might impose non-trivial bias to the learned embeddings”</p>
<p>前面提到，选取负样本的时候不能选择曝光未点击的 hard cases，但是凡事有多个度，当负例中的样本都是很容易就能跟正例区分开的 easy cases，模型也不一定能学得好。因此paper还加入了hard negative。</p>
<p>这里的 hard nagative 指的就是那些与<strong>正例相似性较高</strong>的负例(相对于随机选取的负例)，但是这里的 hard nagative 并不是那些<strong>曝光未点击的负例</strong>，paper 中提出了两种方法来挖掘 hard nagative：online hard negative mining 和 offline hard nagative mining，两种方法的基本流程如下</p>
<h4 id="online-hard-negative-mining"><a href="#online-hard-negative-mining" class="headerlink" title="online hard negative mining"></a>online hard negative mining</h4><p>在每个 batch 的训练中，假设正样本对为 ${(q_(i),d_+^{(i)})}<em>{i=1}^n,$ 则对于每个query$q(i)$， 会从${d</em>+^{(1)}…d_+^{(j)})…d_+^{(n)}|j≠i}$中随机选出 k 个 document 作为 hard nagative，paper 中称其场景下 k=2 是最优的，如果多了会导致模型的效果下降。</p>
<p>paper 中的实验数据表示加入这样的 hard nagative 后，在不同类型的 iterm 的搜索上的召回率均有提升。</p>
<p>但是实际上<strong>以这种方式选取出来的负样本还不够 hard</strong>，原因也很简单，因为这些 negative 是属于不同的 query 的，不同 query 的相关性不高，因此这些样本的相似性也不高，因此有了 offline hard negative mining。</p>
<h4 id="offline-hard-nagative-mining"><a href="#offline-hard-nagative-mining" class="headerlink" title="offline hard nagative mining"></a>offline hard nagative mining</h4><p>offline hard nagative 的做法更像 LTR 的 pairwise 样本构造了，其选取 negative 的方式是在每个 query 的所有 document 中，<strong>选择那些排序在 101-500 的位置的样本作为 hard nagative</strong>；值得注意的是，选择那些 hardest 的 negative 的效果并不是最优的（如排序在第二名的那些）。</p>
<p>综上，在召回样本的选取上，paper 强调了<strong>负样本中的要同时包含 easy nagative 和 hard nagative，paper 的观点是 hard nagative 更关注 non-text 的特征（如 social 特征等），而 easy nagative 则更关注 text 的特征</strong>，因此需要混合两者使用，而混合的方式有两种，分别是</p>
<ol>
<li><strong>blending</strong>, 即混合两者一起来训练，paper 中给出两者的最优比例大概是 easy:hard ≈ 100:1</li>
<li><strong>transfer learning from “hard” model to “easy” model</strong>, 即先用 hard nagative 训练模型，然后用 easy nagative 训练模型（但是 paper 提到从 “easy” model 到 “hard” model并不能达到相同的效果）</li>
</ol>
<h3 id="正样本"><a href="#正样本" class="headerlink" title="正样本"></a>正样本</h3><p>除了负例，paper 中也探索了正例的选择，关于正例的选择做了下面两组的对比</p>
<ul>
<li>选取点击作为正例</li>
<li>选取曝光作为正例</li>
</ul>
<p><strong>实验结果显示在数据量相同的情况下，两者效果基本一致</strong>，即使是在曝光的正例基础上叠加点击正例，结果也没有进一步的提升。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>推荐和搜索的特征上个人认为有较大出入，搜索的特征要涵盖query和document的text feature。而paper中又加入了一些其他的 context feature(paper 中主要提出了两种，location feature 和 social embedding feature) 能取得取得较大提升，下面简单介绍一些这几种 feature。</p>
<h3 id="text-feature"><a href="#text-feature" class="headerlink" title="text feature"></a>text feature</h3><p>对于文本特征的构建，paper 中采用的是 character n-gram 而不是 word n-gram，这里的 n-gram 其实就是把连续的 n 个 character 或 word 作为一个 item 输入到 embedding table 中做 embedding lookup，paper 中通过实验证明了采用 character n-gram 比起 word n-gram 效果要更优，分析其优点如下：</p>
<ol>
<li>embedding lookup table 的 size 更小，能更好的学习到 embedding table 中的参数，其实就是降低了 model size</li>
<li>对于出现在训练集以外的单词有更好的鲁棒性，因为 embedding 的粒度是 character</li>
</ol>
<h3 id="location-feature"><a href="#location-feature" class="headerlink" title="location feature"></a>location feature</h3><p>paper 在 query 和 document 中均添加了了 localtion feature；对于 query，添加的 feature 包括 searcher’s city/region/country/language, 对于 document，则采用一些 publicly available information 如一些 explicit group location 的 tag 之类的。</p>
<h3 id="social-embedding-feature"><a href="#social-embedding-feature" class="headerlink" title="social embedding feature"></a>social embedding feature</h3><p>无详细说明</p>
<h2 id="Serving"><a href="#Serving" class="headerlink" title="Serving"></a>Serving</h2><p>serving 采用的是 ANN（Approximate Near Neighbor），且通过 quantization 来进一步缩短向量间相似性的计算时间，quantization 相当于是一种向量压缩的技术。实际中的<strong>向量化召回系统往往会包含两个步骤，indexing 和 scoring</strong>，indexing 是为了过滤大部分基本无关的候选，而 scoring 则是在相关的候选中进行计算与排序，indexing 常用的技术有 K-means、HNSW、LSH 等，而 scoring 则主要是各种 quantization 及其变种方法。</p>
<p>实际 serving 的时候，<strong>只会实时计算 query 塔的 embedding，而 document 塔的 embedding 则会离线计算好并构建倒排索引</strong>，且在实际的系统中，新的 document 会不断生成，因此还会计算新 document 作为<strong>增量索引</strong>，而间隔一段时间好需要重新计算全量的倒排索引。</p>
<h2 id="Later-stage-Optimization"><a href="#Later-stage-Optimization" class="headerlink" title="Later-stage Optimization"></a>Later-stage Optimization</h2><p>这一部分主要描述了所有推荐系统现在存在的一个 bias，就是训练数据都是由当前系统产生的并反哺给系统的，因此很可能会造成“马太效应”，即强者约强，弱者越弱；更广义来说，这也属于一个 E&amp;E 问题.在 paper 中，这一点体现在<strong>新的ANN召回的结果可能并不会被精排认可</strong>。</p>
<p>为了克服这个问题，paper 中提出了两种方法</p>
<ol>
<li><strong>将召回的 embedding 作为精排模型的特征</strong>，paper 中称这样做的 motivation 是能更快让精排学到新召回的特性；embedding 加入精排作为特征的方式有：embedding 作为 feature 直接加入精排模型、基于 embedding 计算出的值加入精排模型(如 query embedding 与 document embedding 的 cosine similarity）等，其中效果最好是通过 cosine similarity 计算出feature 加入精排模型。</li>
<li>人为干预加入新召回后训练数据的分布。为了避免新召回的结果不被 ANN 认可，paper 中并不仅仅依赖系统自身产生的数据作为训练数据，而是通过人工的方式对被召回的结果重新打上 label，但是在实际中感觉这个操作成本会比较高。</li>
</ol>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11632">Embedding-based Retrieval in Facebook Search</a></p>
<p>[2] <a target="_blank" rel="noopener" href="http://wulc.me/2020/08/30/%E3%80%8AEmbedding-based%20Retrieval%20in%20Facebook%20Search%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Embedding-based Retrieval in Facebook Search》阅读笔记</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/165064102">负样本为王：评Facebook的向量化召回算法</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/152570715">知乎：Embedding-based Retrieval in Facebook Search</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Recall/CF%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AD%98%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Recall/CF%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AD%98%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Collaborative Filtering(CF)是利用集体智慧的一个典型方法。协同过滤相对于集体智慧而言，它从一定程度上保留了个体的特征，就是你的品位偏好，所以它更多可以作为个性化推荐的算法思想。基于内存的协同过滤算法又叫做基于共现关系、基于邻域的协同过滤算法。</p>
<h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>协同过滤算法的计算过程大致分为三步：</p>
<h3 id="1-收集用户偏好"><a href="#1-收集用户偏好" class="headerlink" title="1. 收集用户偏好"></a>1. 收集用户偏好</h3><p>要从<strong>用户的行为和偏好</strong>中发现规律，并基于此给予推荐，如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多方式向系统提供自己的偏好信息，而且不同的应用也可能大不相同：在Feed流推荐系统中，点击、点赞、关注、分享、评论、页面停留时长等都可以作为用户行为；在电商中，点击、收藏、购买等可以作为用户行为。</p>
<p>在一般应用中，我们提取的用户行为一般都多于一种，关于如何组合这些不同的用户行为，基本上有以下两种方式：</p>
<ul>
<li>将不同的行为分组：一般可以分为“查看”和“购买”等等，然后基于不同的行为，计算不同的用户     / 物品相似度。类似于当当网或者 Amazon 给出的“购买了该图书的人还购买了 …”，“查看了图书的人还查看了 …”。</li>
<li>根据不同行为反映用户喜好的程度将它们进行加权，得到用户对于物品的总体喜好。一般来说，显式的用户反馈比隐式的权值大，但比较稀疏，毕竟进行显示反馈的用户是少数；同时相对于“查看”，“购买”行为反映用户喜好的程度更大，但这也因应用而异。</li>
</ul>
<h3 id="2-找到相似的用户或物品"><a href="#2-找到相似的用户或物品" class="headerlink" title="2. 找到相似的用户或物品"></a>2. 找到相似的用户或物品</h3><p>当已经对用户行为进行分析得到用户喜好后，我们可以根据用户喜好计算相似用户和物品，然后基于相似用户或者物品进行推荐，这就是最典型的 CF 的两个分支：基于用户user的 CF 和基于物品item的 CF。</p>
<h4 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h4><p>当已经对用户行为进行分析得到用户喜好后，我们可以根据用户喜好计算相似用户和物品，然后基于相似用户或者物品进行推荐，这就是最典型的 CF 的两个分支：基于用户user的 CF 和基于物品item的 CF。这两种方法都需要计算相似度，下面我们先看看最基本的几种计算相似度的方法。常用的有Cosine 相似度（Cosine Similarity）、欧几里德距离（Euclidean Distance）、皮尔逊相关系数（Pearson Correlation Coefficient）、Tanimoto 系数（Tanimoto Coefficient）也称为 Jaccard 系数等。</p>
<h4 id="相似邻居的计算"><a href="#相似邻居的计算" class="headerlink" title="相似邻居的计算"></a>相似邻居的计算</h4><ul>
<li>固定数量的邻居：K-neighborhoods 或者 Fix-size     neighborhoods</li>
</ul>
<p>不论邻居的“远近”，只取最近的 K 个，作为其邻居。如图 1 中的 A，假设要计算点 1 的 5- 邻居，那么根据点之间的距离，我们取最近的 5 个点，分别是点 2，点 3，点 4，点 7 和点 5。但很明显我们可以看出，这种方法对于孤立点的计算效果不好，因为要取固定个数的邻居，当它附近没有足够多比较相似的点，就被迫取一些不太相似的点作为邻居，这样就影响了邻居相似的程度，比如图 1 中，点 1 和点 5 其实并不是很相似。</p>
<ul>
<li>基于相似度门槛的邻居：Threshold-based     neighborhoods</li>
</ul>
<p>与计算固定数量的邻居的原则不同，基于相似度门槛的邻居计算是对邻居的远近进行最大值的限制，落在以当前点为中心，距离为 K 的区域中的所有点都作为当前点的邻居，这种方法计算得到的邻居个数不确定，但相似度不会出现较大的误差。如图 1 中的 B，从点 1 出发，计算相似度在 K 内的邻居，得到点 2，点 3，点 4 和点 7，这种方法计算出的邻居的相似度程度比前一种优，尤其是对孤立点的处理。</p>
<h3 id="3-计算推荐"><a href="#3-计算推荐" class="headerlink" title="3. 计算推荐"></a>3. 计算推荐</h3><h4 id="基于用户的User-CF"><a href="#基于用户的User-CF" class="headerlink" title="基于用户的User-CF"></a>基于用户的User-CF</h4><p>基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到 K 邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。</p>
<h4 id="基于物品的Item-CF"><a href="#基于物品的Item-CF" class="headerlink" title="基于物品的Item-CF"></a>基于物品的Item-CF</h4><p>基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。 从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。</p>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p><strong>计算复杂度：</strong>Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于<strong>提供商品的电子商务网站</strong>，对于<strong>新闻，博客或者微内容的推荐系统</strong>，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。</p>
<p><strong>适用场景</strong>：在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。UserCF给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算法的原理可以看到，UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF的推荐结果着重于维系用户的历史兴趣。换句话说，<strong>UserCF的推荐更社会化</strong>，反映了用户所在的小型兴趣群体中物品的热门程度，而<strong>ItemCF的推荐更加个性化</strong>，反映了用户自己的兴趣传承。</p>
<h2 id="协同过滤的缺点"><a href="#协同过滤的缺点" class="headerlink" title="协同过滤的缺点"></a>协同过滤的缺点</h2><ol>
<li><p>用户对商品的评价非常稀疏，这样基于用户的评价所得到的用户间的相似性可能不准确（即稀疏性问题）;</p>
</li>
<li><p>随着用户和商品的增多，系统的性能会越来越低;</p>
</li>
<li><p>如果从来没有用户对某一商品加以评价，则这个商品就不可能被推荐（即冷启动问题）。</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Recall/CB%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Recall/CB%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Content-based-Recommendations"><a href="#Content-based-Recommendations" class="headerlink" title="Content-based Recommendations"></a>Content-based Recommendations</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Content-based Recommendations(CB)是根据用户过去喜欢的<strong>产品</strong>（本文统称为 <strong>item</strong>），为用户推荐和他过去喜欢的产品相似的产品。例如，一个推荐饭店的系统可以依据某个用户之前喜欢很多的烤肉店而为他推荐烤肉店。 CB最早主要是应用在信息检索系统当中，所以很多信息检索及信息过滤里的方法都能用于CB中。</p>
<h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>CB的过程一般包括以下三步：</p>
<ol>
<li><p><strong>Item Representation</strong>：为每个item抽取出一些特征（也就是item的content了）来表示此item； 真实应用中的item往往都会有一些可以描述它的属性。这些属性通常可以分为两种：结构化的（structured）属性与非结构化的（unstructured）属性。所谓结构化的属性就是这个属性的意义比较明确，其取值限定在某个范围；而非结构化的属性往往其意义不太明确，取值也没什么限制，不好直接使用。比如在交友网站上，item就是人，一个item会有结构化属性如身高、学历、籍贯等，也会有非结构化属性（如item自己写的交友宣言，博客内容等等）。</p>
</li>
<li><p><strong>Profile Learning</strong>：利用一个用户过去喜欢（及不喜欢）的item的特征数据，来学习出此用户的喜好特征（profile）；假设用户u已经对一些item给出了他的喜好判断，喜欢其中的一部分item，不喜欢其中的另一部分。那么，这一步要做的就是通过用户u过去的这些喜好判断，为他产生一个模型。有了这个模型，我们就可以根据此模型来判断用户u是否会喜欢一个新的item。所以，我们要解决的是一个典型的有监督分类问题，理论上机器学习里的分类算法都可以照搬进这里。</p>
</li>
<li><p><strong>Recommendation Generation</strong>：通过比较上一步得到的用户profile与候选item的特征，为此用户推荐一组相关性最大的item。</p>
</li>
</ol>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="CB的优点"><a href="#CB的优点" class="headerlink" title="CB的优点"></a>CB的优点</h3><ol>
<li><p>用户之间的独立性（User Independence）：既然每个用户的profile都是依据他本身对item的喜好获得的，自然就与他人的行为无关。而CF刚好相反，CF需要利用很多其他人的数据。CB的这种用户独立性带来的一个显著好处是别人不管对item如何作弊（比如利用多个账号把某个产品的排名刷上去）都不会影响到自己。</p>
</li>
<li><p>好的可解释性（Transparency）：如果需要向用户解释为什么推荐了这些产品给他，你只要告诉他这些产品有某某属性，这些属性跟你的品味很匹配等等。</p>
</li>
<li><p>新的item可以立刻得到推荐（New Item Problem）：只要一个新item加进item库，它就马上可以被推荐，被推荐的机会和老的item是一致的。而CF对于新item就很无奈，只有当此新item被某些用户喜欢过（或打过分），它才可能被推荐给其他用户。所以，如果一个纯CF的推荐系统，新加进来的item就永远不会被推荐:( 。</p>
</li>
</ol>
<h3 id="CB的缺点"><a href="#CB的缺点" class="headerlink" title="CB的缺点"></a>CB的缺点</h3><ol>
<li><p>item的特征抽取一般很难（Limited Content Analysis）：如果系统中的item是文档（如个性化阅读中），那么我们现在可以比较容易地使用信息检索里的方法来“比较精确地”抽取出item的特征。但很多情况下我们很难从item中抽取出准确刻画item的特征，比如电影推荐中item是电影，社会化网络推荐中item是人，这些item属性都不好抽。其实，几乎在所有实际情况中我们抽取的item特征都仅能代表item的一些方面，不可能代表item的所有方面。这样带来的一个问题就是可能从两个item抽取出来的特征完全相同，这种情况下CB就完全无法区分这两个item了。比如如果只能从电影里抽取出演员、导演，那么两部有相同演员和导演的电影对于CB来说就完全不可区分了。</p>
</li>
<li><p>无法挖掘出用户的潜在兴趣（Over-specialization）：既然CB的推荐只依赖于用户过去对某些item的喜好，它产生的推荐也都会和用户过去喜欢的item相似。如果一个人以前只看与推荐有关的文章，那CB只会给他推荐更多与推荐相关的文章，它不会知道用户可能还喜欢数码。</p>
</li>
<li><p>无法为新用户产生推荐（New User Problem）：新用户没有喜好历史，自然无法获得他的profile，所以也就无法为他产生推荐了。当然，这个问题CF也有。</p>
</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CB应该算是第一代的个性化应用中最流行的推荐算法了。但由于它本身具有某些很难解决的缺点，再加上在大多数情况下其精度都不是最好的，目前大部分的推荐系统都是以其他算法为主（如CF），而辅以CB以解决主算法在某些情况下的不精确性（如解决新item问题）。但CB的作用是不可否认的，只要具体应用中有可用的属性，那么基本都能在系统里看到CB的影子。组合CB和其他推荐算法的方法很多（我很久以后会写一篇博文详细介绍之），最常用的可能是用CB来过滤其他算法的候选集，把一些不太合适的候选（比如不要给小孩推荐偏成人的书籍）去掉。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Rank/YouTubeMultiTask/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Rank/YouTubeMultiTask/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Recommending-what-video-to-watch-next-a-multitask-ranking-system"><a href="#Recommending-what-video-to-watch-next-a-multitask-ranking-system" class="headerlink" title="Recommending what video to watch next: a multitask ranking system"></a>Recommending what video to watch next: a multitask ranking system</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>这篇论文提出一个视频推荐领域的大规模的多目标排序系统，该系统主要面临几个挑战：1）多个竞争关系的目标；2）用户反馈的选择偏见（selective bias）。 本文探究了大量软-参数共享技术，例如MMoE，来有效的优化多目标排序。除此之外，本文还采用wide&amp;deep框架来缓和选择偏见问题。并且在youtube线上环境验证了方案的效果。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本文描述了一个用户视频推荐的大规模排序服务。具体场景是：给定一个用户播放的视频，生成下一个他可能会播放和喜欢的视频。 典型的推荐系统包含两个阶段：召回（recall），排序(rank)。本文的重点聚焦在排序阶段：对召回阶段输出的数百个内容，应用复杂的模型进行排序，选出其中最有可能被用户喜欢的。</p>
<p>设计与实现一个大规模的线上推荐系统会面临一下挑战：</p>
<ul>
<li>经常面临不同，甚至冲突的多个目标</li>
<li>经常碰到隐式偏见，例如用户倾向于点击排序靠前的视频，而不是最好的。如果训练数据处理的时候不考虑到这些因素，会陷入feedback loop effect。</li>
</ul>
<p>本文的贡献：</p>
<ul>
<li>提出一个基于deep&amp;wide框架的多任务学习框架</li>
<li>deep部分是一个MMoE多任务学习框架</li>
<li>wide部分引入一个浅层网络，来降低selection bias(这篇论文用的是position bias)</li>
</ul>
<p>作者首先把目标分为两类：</p>
<ul>
<li>参与性 点击、播放等隐式行为</li>
<li>满足性 点赞、打分等显示行为</li>
</ul>
<p>这些目标可能正相关，也可能负相关，正好适合应用MMoE框架来解决。</p>
<p>为了对selection bias进行建模，引入了一个浅层的模型。该模型输入是一个能刻画selection bias的因子，例如排序的位置，输出一个标量作为主模型输出的偏移量。简而言之，该模型把用户对视频的喜好拆分成两部分：无偏效用（MMoE学习），bias（浅层网络学习）</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>推荐问题可以理解为：给定一个查询、上下文、候选集，返回一个高可用性的小列表。在这一节，我们会分三部分来讨论：1）工业推荐系统的样例分析；2）多目标排序系统；3）理解训练数据的偏倚问题。</p>
<h3 id="工业级推荐系统"><a href="#工业级推荐系统" class="headerlink" title="工业级推荐系统"></a>工业级推荐系统</h3><p>想要设计、开发一个强大机器学习模型加持的成功的排序系统，我们需要大量的训练数据，在大多数现有的推荐系统中，训练数据都依赖用户的行为日志。推荐系统想获得用户的显式反馈，例如电影评分等，但是由于用户成本很高，所以显示反馈往往很稀疏。因此，目前推荐系统的训练大多数依赖用户的隐式反馈，例如点击、播放等等。<br>推荐系统一般分为两个阶段：</p>
<ol>
<li>召回：从海量视频中筛选出部分用户感兴趣的视频，为了提高多样性召回一般使用多路。通常包括：利用共现关系（关联规则），协同过滤，随机游走，基于内容，混合方法。</li>
<li>排序：一般采用LearningToRank方法，包括point-wise, pair-wise, list-wise。线上排序服务重点需要考虑效率问题，所以本文选用基于深度神经网络的point-wise方法，因为可扩展性强。</li>
</ol>
<h3 id="推荐系统中的多目标学习"><a href="#推荐系统中的多目标学习" class="headerlink" title="推荐系统中的多目标学习"></a>推荐系统中的多目标学习</h3><p>在推荐系统中，用户的行为多种多样，例如：点击、点赞、下单等。 单个用户行为并不能准确反映用户对Item的好恶，例如一个用户播放某个视频，但是最后给了一个低分。并且这些行为之间不是相互独立的，可能会结合在一起决定用户对视频的偏好。所以，我们要结合这些行为分数在一起来评价用户的对某个视频的偏好。<br>目前不少推荐系统都有考虑到多目标。例如大多数推荐系统在召回阶段都会考虑到多目标，因此应用多种算法来做召回。 还有一些针对特定特征做了多目标排序，例如针对文本特征做了排序，针对图像特征做了一个排序。这些往系统的可扩展性比较差，一来没有利用不同特征之间的关系，二来一旦特征空间比较复杂，整个排序的参数规模会很大。</p>
<h3 id="位置偏倚"><a href="#位置偏倚" class="headerlink" title="位置偏倚"></a>位置偏倚</h3><p>用户隐式反馈受位置偏倚影响很大，这个在搜索和推荐领域早就有人证明了这点。因此，也有了一些工作希望移除位置偏倚的影响。比较常用的做法是把位置作为一个参数带入模型训练和预测过程。例如把位置作为条件概率的前键值，对$p(click|position,item)$进行建模；在预测阶段，计算$p(click|position=1, item)$移除位置的影响。也有一些工作，学习一个全局的$bias$因子来对结果进行正则化，缺点是这个$bias$做不到个性化。而且，在真实的推荐系统中，用户的兴趣偏好和视频的流行度每天都在变化，因此利用全局的$bias$很难取得好效果。</p>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>综上所述，推荐领域的排序面临以下几个问题：</p>
<ul>
<li>隐式反馈不能代表用户真实兴趣</li>
<li>需要考虑多目标</li>
<li>特征很复杂，覆盖很多垂直领域的特征</li>
<li>位置偏倚</li>
<li>大规模应用场景要求模型算法的可扩展性</li>
</ul>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="http://guoshihao.site:8085/pic/YouTubeMultitask0.png"></p>
<h3 id="排序目标"><a href="#排序目标" class="headerlink" title="排序目标"></a>排序目标</h3><p>本文使用用户的行为种类作为多目标分类的依据。把用户的行为分为参与性和满足性两类。其中参与性拆分成两个目标：一个二分类（是否点击），一个回归（时长）；满足性也可以拆分成两个目标：二分类（是否点赞）、回归（评分）。相应的损失函数分别为：交叉熵损失和平方损失。</p>
<p>一个输入样本，模型会输出多个目标的得分。一个权重计算公式结合多个得分计算出最终的分数。这个权重是参数调节出来的，并不是模型学习的。</p>
<h3 id="MMoE建模"><a href="#MMoE建模" class="headerlink" title="MMoE建模"></a>MMoE建模</h3><p>多目标排序模型通常的结构都是底层共享一个shared-bottom结构。然而，如果这些底层共享比较硬，对于相互关系不密切，甚至矛盾的多目标建模，最终的效果会收到损害。因此，youtube在2018年提出了MMoE结构，bottom层为多个expert模块组合而成，每个任务的组合系统不同，因此参数共享比较soft。</p>
<p><img src="http://guoshihao.site:8085/pic/YouTubeMultitask1.png"></p>
<p>该模型结构并不完全等同于MMoE，因为在推荐领域特征规模很大，为了降低复杂度，在MoE层和输入之间插入一个shared bottom层来对特征进行降维。这种方法其实很常见，例如可以对底层的embedding matrix进行卷积操作，降低矩阵的大小。</p>
<p>shared bottom层上面，就是MoE层，以及边缘的gate。每个MoE层只是一层MLP+ReLU。给定输入$x$，MoE层的输出为$f_i(x),(i=1,…,n)$是n个expert network（expert network可认为是一个神经网络）。每个任务$k$的tower的输入为MoE的混合(Mixture), 记为$f^k(x)=\sum^n_{i=1}g^k(x)<em>if_i(x)$，每个任务有一个独立的$g^k(x)$，即有一个独立的gating network。每个$g^k(x)=softmax(W_{gk}x+b</em>{gk}),W_{gk}\in\mathbb{R}^{n \times d},b_{gk}\in\mathbb{R}^{n \times 1}$。Expert的数量可以很多，但是每个task最终采纳的可能只有权重最大的$TopK$个。</p>
<h3 id="建模并移除position和selective-bias"><a href="#建模并移除position和selective-bias" class="headerlink" title="建模并移除position和selective bias"></a>建模并移除position和selective bias</h3><p>隐式反馈的位置偏倚，以及其他类型的选择性偏倚在推荐和广告系统中广泛存在，因为隐式反馈是来自于本系统的上一次推荐，这就是feedback loop。例如在推荐系统中，我们想基于用户目前正在观看的视频，预测他下一个观看的视频，用户往往倾向于点击观看列表最上面的视频，这和他的兴趣偏好之类因素无关。我们的目标就是在排序模型中移除这种位置偏倚，来提高模型的质量，中断feedback loop。</p>
<p>postion和设备特征一起，通过一个浅层网络输出的logit会和task specific tower的logit结合在一起，预测最终的输出。在训练阶段，只有部分样本会加入位置信息，这样是为了防止模型过拟合位置信息。在预测阶段，不考虑位置特征，或者位置特征取一样。模型结构大约如图所示：</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>实验表明：</p>
<ul>
<li>某些expert的作用比其他expert大；</li>
<li>一些task分配给Expert的权重较为均匀，另一些则更极端一些。极端情况的出现会导致gating网络不稳定，一般引入dropout来降低影响。</li>
</ul>
<p>一些在其他领域取得不错效果的模型，如CNN, multi-head attention在CTR预估领域并不能取得很好的效果。原因可能是包括：特征多模态，推荐用到的特征模态比较复杂；扩展性和多目标冲突；噪音和数据稀疏性。</p>
<p>出于效率考虑，网络结构不易太复杂太深。</p>
<p>训练数据还存在其他bias。</p>
<p>复杂模型线下，和线上效果评估可能差别较大。</p>
<p>[1] <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3298689.3346997">Recommending what video to watch next: a multitask ranking system</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://blog.csdn.net/fanzitao/article/details/104525843/">多任务学习论文导读：Recommending What Video to Watch Next-A Multitask Ranking System</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Rank/Wide&Deep/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Rank/Wide&Deep/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Wide-amp-Deep-Learning-for-Recommender-Systems"><a href="#Wide-amp-Deep-Learning-for-Recommender-Systems" class="headerlink" title="Wide &amp; Deep Learning for Recommender Systems"></a>Wide &amp; Deep Learning for Recommender Systems</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Wide &amp; Deep 模型的核心思想是结合线性模型的记忆能力和 DNN 模型的泛化能力，从而提升整体模型性能。Wide &amp; Deep 已成功应用到了 Google Play 的app推荐业务，并于TensorFlow中封装。该结构被提出后即引起热捧，在业界影响力非常大，很多公司纷纷仿照该结构并成功应用于自身的推荐等相关业务。推荐系统的主要挑战之一，是同时解决Memorization和Generalization。</p>
<h3 id="Memorization"><a href="#Memorization" class="headerlink" title="Memorization"></a>Memorization</h3><p>面对拥有大规模离散sparse特征的CTR预估问题时，将特征进行非线性转换，然后再使用线性模型是在业界非常普遍的做法，最流行的即「LR+特征叉乘」。Memorization 通过一系列<strong>人工的特征叉乘（cross-product）</strong>来构造这些非线性特征，捕捉sparse特征之间的高阶相关性，即“记忆” 历史数据中曾共同出现过的特征对。典型代表是LR模型，使用大量的原始sparse特征和<strong>叉乘特征</strong>作为输入，很多原始的dense特征通常也会被分桶离散化构造为sparse特征。</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>模型可解释高，实现快速高效，特征重要度易于分析，在工业界已被证明是很有效的。</p>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li><p>需要更多的人工设计；</p>
</li>
<li><p>可能出现过拟合。可以这样理解：如果将所有特征叉乘起来，那么几乎相当于纯粹记住每个训练样本，这个极端情况是最细粒度的叉乘，我们可以通过构造更粗粒度的特征叉乘来增强泛化性；</p>
</li>
<li><p>无法捕捉训练数据中未曾出现过的特征对。</p>
</li>
</ol>
<h3 id="Generalization"><a href="#Generalization" class="headerlink" title="Generalization"></a>Generalization</h3><p>Generalization 为sparse特征学习低维的dense embeddings 来捕获特征相关性，学习到的embeddings 本身带有一定的语义信息。可以联想到NLP中的词向量，不同词的词向量有相关性，因此文中也称Generalization是基于相关性之间的传递。这类模型的代表是DNN和FM。</p>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><p>深度神经网络模型无需人工构建组合特征，有自动做特征组合的能力，对历史上没有出现的特征组合有更好的泛化性 。而且由于加入非线性的激活函数，神经网络模型具备非线性建模能力。</p>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><p>但在当user-item矩阵非常稀疏时，例如有和独特爱好的users以及很小众的items，NN很难为users和items学习到有效的embedding。这种情况下，大部分user-item应该是没有关联的，但dense embedding 的方法还是可以得到对所有 user-item pair的非零预测，因此导致 over-generalize并推荐不怎么相关的物品。此时Memorization就展示了优势，它可以“记住”这些特殊的特征组合。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>Wide &amp; Deep模型结合了LR和DNN，其框架图如下所示。</p>
<p><img src="http://guoshihao.site:8085/pic/wide%26deep.png"></p>
<h3 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h3><p>该部分是广义线性模型，即$y=w^T[x,\phi(x)]+b$ ，其中$x$和$\phi(x)$表示原始特征与叉乘特征。</p>
<h3 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h3><p>该部分是前馈神经网络，网络会对一些sparse特征（如ID类特征）学习一个低维的dense embeddings（维度量级通常在$O(10)$到$O(100)$之间），然后和一些原始dense特征一起作为网络的输入。</p>
<p>$a^{l+1}=f(W^la^l+b^l)$，其中$a^l,b^l,W^l$是第$l$层的激活值，偏置和权重，$f$是激活函数。</p>
<p>损失函数选取Logistic Loss作为损失函数，此时模型最后的预测输出为：<br>$$<br>p(y=1|x)=\sigma(w^T_{wide}[x,\phi(x)]+w^T_{deep}a^{l_f}+b)<br>$$<br>其中$\sigma$是sigmoid函数，$\phi(x)$表示叉乘特征，$a^{l_f}$表示神经网络最后一层激活值。</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>联合训练（Joint Training）和集成（Ensemble）是不同的，集成是每个模型单独训练，再将模型的结果汇合。相比联合训练，集成的每个独立模型都得学得足够好才有利于随后的汇合，因此每个模型的model size也相对更大。而联合训练的wide部分只需要作一小部分的特征叉乘来弥补deep部分的不足，不需要 一个full-size 的wide 模型。</p>
<p>在论文中，作者通过梯度的反向传播，使用 mini-batch stochastic optimization 训练参数，并对wide部分使用带L1正则的Follow- the-regularized-leader (FTRL) 算法，对deep部分使用 AdaGrad算法。</p>
<h2 id="线上实验"><a href="#线上实验" class="headerlink" title="线上实验"></a>线上实验</h2><h3 id="实验场景"><a href="#实验场景" class="headerlink" title="实验场景"></a>实验场景</h3><p>Google Play商店的app推荐中，当一个user访问Google Play，会生成一个包含user和contextual信息的query，推荐系统的精排模型会对于候选池中召回的一系列apps（即item，文中也称 impression）进行打分，按打分生成app的排序列表返回给用户。Deep&amp;Wide对应这里的精排模型，输入包括&lt;user，contextual，impression&gt;的信息 。</p>
<h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><ul>
<li>训练样本约5000亿</li>
<li>Categorical 特征（sparse）会有一个过滤阈值，即至少在训练集中出现m次才会被加入</li>
<li>Continuous 特征（dense）通过CDF被归一化到 [0,1] 之间</li>
<li>Categorical 特征映射到32维embeddings，和原始Continuous特征共1200维作为NN输入</li>
<li>Wide部分只用了一组特征叉乘，即<strong>被推荐的app ☓ 用户下载的app</strong></li>
<li>线上模型更新时，通过热启动重训练，即使用上次的embeddings和模型参数初始化</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Wide的存在意义：Wide部分的作用是记忆，其设计动机是假设大部分用户的行为还是具有显然的，直接的，规律可循，有一些强的关联规则，从GooglePlay实验的Wide特征的选取也可以看出来，Wide部分和特征、业务场景的关联性比较大。</p>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53361519">详解 Wide &amp; Deep 结构背后的动机</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Rank/todo-%E9%98%BF%E9%87%8C%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Rank/todo-%E9%98%BF%E9%87%8C%E6%A6%82%E8%BF%B0/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54822778">https://zhuanlan.zhihu.com/p/54822778</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Rank/todo-BST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Rank/todo-BST/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/20/RecommendSystem/Rank/PNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="没有知识的荒原">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/20/RecommendSystem/Rank/PNN/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-20T10:32:53+08:00">
                2021-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Product-based-Neural-Networks-for-User-Response-Prediction"><a href="#Product-based-Neural-Networks-for-User-Response-Prediction" class="headerlink" title="Product-based Neural Networks for User Response Prediction"></a>Product-based Neural Networks for User Response Prediction</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>通过PNN来学习类别特征的Embedding表示，<strong>Product</strong>层用来捕获层间类别的交互模式，用更深的全连接层来进一步提取高阶交互特征。</p>
<p>文章指出<strong>FNN</strong>的弊端：</p>
<ol>
<li><p>FM限制了Embedding的质量；</p>
</li>
<li><p>“add”操作不足以提取出多个field的交互特征；Product layer部分的思想是认为特征之间的关系是and“且”的一种关系，而非add”加”的关系。</p>
</li>
</ol>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>PNN模型结合了FM和DNN，其框架图如下所示。</p>
<p><img src="http://guoshihao.site:8085/pic/PNN.png"></p>
<p>可以看出模型也是实现CTR预估任务，输入数据是对特征进行one-hot编码之后特征，然后经过一个embedding层，将输入特征映射到相同长度的特征得到上图中的embedding层，接下来就是Product Layer了。Product layer主要有两部分，一部分是线性部分z，另一部分是非线性组合部分p。得到这一部分的特征之后，就进行常规的深度学习部分了，经过两个隐藏层，然后直接输入CTR结果。本文的重点是Product Layer部分。</p>
<h3 id="Porduct-Layer"><a href="#Porduct-Layer" class="headerlink" title="Porduct Layer"></a>Porduct Layer</h3><p>Product layer部分的思想是认为特征之间的关系是and“且”的一种关系，而非add”加”的关系。以往的方法中特征之间都是$$w_{i}x_{i}+w_{j}x_{j}$$的组合方式，这种组合方式过于简单而且是不能凸显出特征之间紧密关系。</p>
<p>再看上图，product layer部分包含两部分，第一部分是z，第二部分是p。虽然这一层的特征都是圆圈来表示，但是意义却大不相同。</p>
<p>z部分是线性部分，相当与把Embedding层的向量concat成一个向量即可。所以z中的每一个圆圈代表的是一个向量（每一个field特征的embedding表示）。<br>$$<br>z=concat([emb_1,emb_2..,emb_n],axis=1)<br>$$<br>p部分是交叉部分，有两种表示分别是内积和外积。内积用IPNN表示，外积用OPNN表示。</p>
<h4 id="IPNN-Inner-Product-based-Neural-Network"><a href="#IPNN-Inner-Product-based-Neural-Network" class="headerlink" title="IPNN(Inner Product-based Neural Network)"></a>IPNN(Inner Product-based Neural Network)</h4><p>IPNN就是实现两个向量的点乘，得到的是一个数，此时p部分的每个圆圈表示一个数。思路就是将所有field的embedding表示两两相乘，可以得到一个长为$$field_size*(field_size-1)/2$$的向量，然后和z部分的向量拼接起来，送入MLP。</p>
<h4 id="OPNN-Outer-Product-based-Neural-Network"><a href="#OPNN-Outer-Product-based-Neural-Network" class="headerlink" title="OPNN(Outer Product-based Neural Network)"></a>OPNN(Outer Product-based Neural Network)</h4><p>OPNN就是实现两个向量的外积，得到一个矩阵，此时p部分的每个圆圈表示一个矩阵。为了实现矩阵与向量的拼接，作者引入了一个可学习的矩阵$$W$$。进行如下的表达式运算：<br>$$<br>u^Tv*W=u\times{}W\times{}v^T<br>$$<br>其中$$u,v$$均为$$[1，embedding_size]$$的向量，$$W$$为$$[embedding_size,embedding_size]$$的矩阵。</p>
<p>思路就是通过向量的外积运算得到$$field_size*(field_size-1)/2$$个维度为$$[embedding_size,embedding_size]$$的矩阵，然后这些矩阵分别和$$W$$做矩阵内积相加得到$$field_size*(field_size-1)/2$$个数，</p>
<p>所以不管是IPNN和OPNN，送入隐藏层的特征长度为线性部分z的长度加非线性部分p的长度$$field_size<em>embedding_size+field_size</em>(field_size-1)/2$$，最终就是常规的MLP输出要预测的指标。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>和FNN相比，PNN多了一个<strong>product层</strong>，和FM相比，PNN多了MLP层，并且输出不是简单的叠加，而是通过更复杂的Product Layer；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM；</p>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.00144">Product-based Neural Networks for User Response Prediction</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/2/">&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">111</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guo SHihao</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
